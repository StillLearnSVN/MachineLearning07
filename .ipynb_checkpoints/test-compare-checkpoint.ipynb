{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d27a0fc-ba58-4e68-9339-086f6d98f724",
   "metadata": {},
   "source": [
    "## Comparative Clustering Analysis: K-Means vs DBSCAN\n",
    "\n",
    "Key Differences Between K-Means and DBSCAN:\n",
    "\n",
    "K-Means: Requires predefined number of clusters\n",
    "\n",
    "DBSCAN: Discovers clusters based on density\n",
    "\n",
    "K-Means: Assumes spherical cluster shapes\n",
    "\n",
    "DBSCAN: Can find arbitrarily shaped clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2095a9-7365-4479-b1c1-8fdcc3c09cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Processing and ML Libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Clustering Models\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Additional Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class ComparativeClustering:\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize the comparative clustering analysis\n",
    "        \n",
    "        Parameters:\n",
    "        file_path (str): Path to the Excel file\n",
    "        \"\"\"\n",
    "        # Download NLTK resources\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        self.load_and_preprocess_data(file_path)\n",
    "    \n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess data from Excel file\n",
    "        \"\"\"\n",
    "        # Read Excel sheets\n",
    "        ib_data = pd.read_excel(file_path, sheet_name='DATA IB')\n",
    "        ik_data = pd.read_excel(file_path, sheet_name='DATA IK')\n",
    "        \n",
    "        # Combine purposes\n",
    "        ib_data['source'] = 'IB'\n",
    "        ik_data['source'] = 'IK'\n",
    "        ib_data['purpose'] = ib_data['desc']\n",
    "        ik_data['purpose'] = ik_data['tujuan']\n",
    "        \n",
    "        # Merge datasets\n",
    "        self.combined_data = pd.concat([\n",
    "            ib_data[['purpose', 'source']], \n",
    "            ik_data[['purpose', 'source']]\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Remove NaN values\n",
    "        self.combined_data = self.combined_data.dropna(subset=['purpose'])\n",
    "    \n",
    "    def advanced_text_preprocessing(self, text):\n",
    "        \"\"\"\n",
    "        Advanced text preprocessing\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('indonesian'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def feature_extraction(self):\n",
    "        \"\"\"\n",
    "        Extract features from text data\n",
    "        \n",
    "        Returns:\n",
    "        numpy array: Feature matrix\n",
    "        list: Feature names\n",
    "        \"\"\"\n",
    "        # Preprocess text\n",
    "        self.combined_data['processed_purpose'] = self.combined_data['purpose'].apply(self.advanced_text_preprocessing)\n",
    "        \n",
    "        # Feature extraction using TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        feature_matrix = vectorizer.fit_transform(self.combined_data['processed_purpose'])\n",
    "        \n",
    "        # Store feature names for interpretation\n",
    "        self.feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        return feature_matrix.toarray(), self.feature_names\n",
    "    \n",
    "    def perform_clustering(self, features, feature_names):\n",
    "        \"\"\"\n",
    "        Perform comparative clustering analysis\n",
    "        \n",
    "        Parameters:\n",
    "        features (numpy array): Feature matrix\n",
    "        feature_names (list): Feature names\n",
    "        \n",
    "        Returns:\n",
    "        dict: Clustering results for K-Means and DBSCAN\n",
    "        \"\"\"\n",
    "        # Dimensionality reduction for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_features = pca.fit_transform(features)\n",
    "        \n",
    "        # Clustering methods to compare\n",
    "        clustering_methods = {\n",
    "            'K-Means': {\n",
    "                'algorithm': KMeans(n_clusters=5, random_state=42, n_init=10),\n",
    "                'params': {'n_clusters': range(2, 11)}\n",
    "            },\n",
    "            'DBSCAN': {\n",
    "                'algorithm': DBSCAN(eps=0.5, min_samples=3),\n",
    "                'params': {\n",
    "                    'eps': [0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "                    'min_samples': [2, 3, 5]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Results storage\n",
    "        clustering_results = {}\n",
    "        \n",
    "        # Comparative analysis\n",
    "        for method_name, method_config in clustering_methods.items():\n",
    "            print(f\"\\nAnalyzing {method_name} Clustering:\")\n",
    "            \n",
    "            if method_name == 'K-Means':\n",
    "                # K-Means specific evaluation\n",
    "                silhouette_scores = []\n",
    "                for n_clusters in method_config['params']['n_clusters']:\n",
    "                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                    cluster_labels = kmeans.fit_predict(features)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    silhouette = silhouette_score(features, cluster_labels)\n",
    "                    silhouette_scores.append(silhouette)\n",
    "                \n",
    "                # Visualize K-Means cluster evaluation\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(list(method_config['params']['n_clusters']), silhouette_scores, marker='o')\n",
    "                plt.title(f'{method_name} Silhouette Scores')\n",
    "                plt.xlabel('Number of Clusters')\n",
    "                plt.ylabel('Silhouette Score')\n",
    "                plt.show()\n",
    "                \n",
    "                # Select best number of clusters\n",
    "                best_n_clusters = list(method_config['params']['n_clusters'])[np.argmax(silhouette_scores)]\n",
    "                kmeans = KMeans(n_clusters=best_n_clusters, random_state=42, n_init=10)\n",
    "                cluster_labels = kmeans.fit_predict(features)\n",
    "            \n",
    "            else:  # DBSCAN\n",
    "                # DBSCAN specific evaluation\n",
    "                best_silhouette = -1\n",
    "                best_params = {}\n",
    "                best_cluster_labels = None\n",
    "                \n",
    "                for eps in method_config['params']['eps']:\n",
    "                    for min_samples in method_config['params']['min_samples']:\n",
    "                        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                        cluster_labels = dbscan.fit_predict(features)\n",
    "                        \n",
    "                        # Only evaluate if more than one cluster is found\n",
    "                        unique_clusters = np.unique(cluster_labels)\n",
    "                        if len(unique_clusters) > 1 and -1 not in unique_clusters:\n",
    "                            try:\n",
    "                                silhouette = silhouette_score(features, cluster_labels)\n",
    "                                if silhouette > best_silhouette:\n",
    "                                    best_silhouette = silhouette\n",
    "                                    best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "                                    best_cluster_labels = cluster_labels\n",
    "                            except:\n",
    "                                continue\n",
    "                \n",
    "                cluster_labels = best_cluster_labels\n",
    "                print(f\"Best DBSCAN Parameters: {best_params}\")\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.subplot(121)\n",
    "            scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                                  c=cluster_labels, cmap='viridis')\n",
    "            plt.title(f'{method_name} Clustering')\n",
    "            plt.colorbar(scatter)\n",
    "            \n",
    "            # Cluster distribution\n",
    "            plt.subplot(122)\n",
    "            cluster_counts = pd.Series(cluster_labels).value_counts()\n",
    "            cluster_counts.plot(kind='bar')\n",
    "            plt.title(f'{method_name} Cluster Distribution')\n",
    "            plt.xlabel('Cluster')\n",
    "            plt.ylabel('Number of Samples')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Generate business insights\n",
    "            clustering_results[method_name] = self.generate_business_insights(\n",
    "                features, cluster_labels, feature_names\n",
    "            )\n",
    "        \n",
    "        return clustering_results\n",
    "    \n",
    "    def generate_business_insights(self, features, cluster_labels, feature_names):\n",
    "        \"\"\"\n",
    "        Generate business insights from clustered data\n",
    "        \"\"\"\n",
    "        # Add cluster labels to dataframe\n",
    "        self.combined_data['cluster'] = cluster_labels\n",
    "        \n",
    "        # Analyze clusters\n",
    "        cluster_insights = {}\n",
    "        unique_clusters = np.unique(cluster_labels)\n",
    "        \n",
    "        for cluster in unique_clusters:\n",
    "            if cluster == -1:  # Skip noise points in DBSCAN\n",
    "                continue\n",
    "            \n",
    "            cluster_data = self.combined_data[self.combined_data['cluster'] == cluster]\n",
    "            \n",
    "            # Find top keywords for the cluster\n",
    "            cluster_features = features[cluster_labels == cluster]\n",
    "            top_keyword_indices = cluster_features.sum(axis=0).argsort()[::-1][:5]\n",
    "            top_keywords = [feature_names[i] for i in top_keyword_indices]\n",
    "            \n",
    "            cluster_insights[cluster] = {\n",
    "                'keywords': top_keywords,\n",
    "                'sample_purposes': cluster_data['purpose'].sample(min(3, len(cluster_data))).tolist(),\n",
    "                'business_opportunities': self.map_keywords_to_opportunities(top_keywords)\n",
    "            }\n",
    "        \n",
    "        return cluster_insights\n",
    "    \n",
    "    def map_keywords_to_opportunities(self, keywords):\n",
    "        \"\"\"\n",
    "        Map keywords to potential business opportunities\n",
    "        \"\"\"\n",
    "        opportunity_mappings = {\n",
    "            'laptop': ['Computer Repair Shop', 'Laptop Sales and Service', 'Tech Accessory Store'],\n",
    "            'orang': ['Family Support Services', 'Travel Assistance', 'Family Reunion Planning'],\n",
    "            'market': ['Market Research Consultancy', 'Local Business Consulting', 'Research Services'],\n",
    "            'ibadah': ['Event Management', 'Religious Event Planning', 'Community Event Services'],\n",
    "            'dinas': ['Government Liaison Services', 'Permit and Documentation Assistance']\n",
    "        }\n",
    "        \n",
    "        opportunities = []\n",
    "        for keyword in keywords:\n",
    "            if keyword in opportunity_mappings:\n",
    "                opportunities.extend(opportunity_mappings[keyword])\n",
    "        \n",
    "        return list(set(opportunities))\n",
    "\n",
    "def main():\n",
    "    # File path\n",
    "    file_path = 'dataset.xlsx'\n",
    "    \n",
    "    # Initialize comparative clustering analysis\n",
    "    analysis = ComparativeClustering(file_path)\n",
    "    \n",
    "    # Feature Extraction\n",
    "    features, feature_names = analysis.feature_extraction()\n",
    "    \n",
    "    # Perform Comparative Clustering\n",
    "    clustering_results = analysis.perform_clustering(features, feature_names)\n",
    "    \n",
    "    # Print Business Insights\n",
    "    for clustering_method, insights in clustering_results.items():\n",
    "        print(f\"\\n--- {clustering_method} Clustering Business Insights ---\")\n",
    "        for cluster, cluster_info in insights.items():\n",
    "            print(f\"\\nCluster {cluster}:\")\n",
    "            print(f\"Top Keywords: {cluster_info['keywords']}\")\n",
    "            print(\"Sample Purposes:\")\n",
    "            for purpose in cluster_info['sample_purposes']:\n",
    "                print(f\"- {purpose}\")\n",
    "            print(\"Potential Business Opportunities:\")\n",
    "            for opportunity in cluster_info['business_opportunities']:\n",
    "                print(f\"- {opportunity}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18610e-711c-48f6-96ec-d1f658966701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
