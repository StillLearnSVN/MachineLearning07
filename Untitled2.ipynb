{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ff4306-3d84-45cf-a663-d16469cb0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, silhouette_score\n",
    "\n",
    "# Keras/TensorFlow imports\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88976ee7-ddbe-4cb1-958b-4798bf7f58b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined data: 80366\n",
      "TF-IDF Matrix shape: (80366, 500)\n"
     ]
    }
   ],
   "source": [
    "class AdvancedMLProject:\n",
    "    def __init__(self, file_path):\n",
    "        # Load data with error handling\n",
    "        try:\n",
    "            self.ib_data = pd.read_excel(file_path, sheet_name=\"DATA IB\")\n",
    "            self.ik_data = pd.read_excel(file_path, sheet_name=\"DATA IK\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Excel file: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Preprocess data\n",
    "        self.prepare_data()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # Combine and preprocess data with error handling\n",
    "        try:\n",
    "            # Use the correct column names based on your description\n",
    "            ib_data = self.ib_data[[\"desc\"]].rename(columns={\"desc\": \"keperluan\"})\n",
    "            ik_data = self.ik_data[[\"tujuan\"]].rename(columns={\"tujuan\": \"keperluan\"})\n",
    "            \n",
    "            ib_data[\"izin\"] = \"IB\"\n",
    "            ik_data[\"izin\"] = \"IK\"\n",
    "            \n",
    "            # Combine datasets\n",
    "            self.combined_data = pd.concat([ib_data, ik_data], ignore_index=True)\n",
    "            \n",
    "            # Text preprocessing with additional cleaning\n",
    "            self.combined_data['keperluan_cleaned'] = (\n",
    "                self.combined_data['keperluan']\n",
    "                .str.lower()\n",
    "                .str.strip()\n",
    "                .str.replace('[^\\w\\s]', '', regex=True)  # Remove punctuation\n",
    "            )\n",
    "            \n",
    "            # Encode permit type\n",
    "            le = LabelEncoder()\n",
    "            self.combined_data['izin_encoded'] = le.fit_transform(self.combined_data['izin'])\n",
    "            \n",
    "            print(f\"Total combined data: {len(self.combined_data)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data preparation: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def feature_extraction(self, max_features=500):\n",
    "        # More robust TF-IDF Vectorization\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                max_features=max_features,\n",
    "                stop_words='english',  # You might want to use Indonesian stop words\n",
    "                ngram_range=(1, 2)  # Consider unigrams and bigrams\n",
    "            )\n",
    "            tfidf_matrix = vectorizer.fit_transform(self.combined_data['keperluan_cleaned']).toarray()\n",
    "            \n",
    "            print(f\"TF-IDF Matrix shape: {tfidf_matrix.shape}\")\n",
    "            return tfidf_matrix, vectorizer\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature extraction: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def multiple_clustering_approaches(self, tfidf_matrix):\n",
    "        # More robust clustering with error handling\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            # K-means Clustering with multiple initializations\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=5, \n",
    "                n_init=10,  # Multiple initializations to find best centroid\n",
    "                random_state=42\n",
    "            )\n",
    "            kmeans_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "            results['KMeans'] = {\n",
    "                'labels': kmeans_labels,\n",
    "                'silhouette_score': silhouette_score(tfidf_matrix, kmeans_labels)\n",
    "            }\n",
    "            \n",
    "            # DBSCAN with adaptive parameters\n",
    "            try:\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "                scaled_matrix = StandardScaler().fit_transform(tfidf_matrix)\n",
    "                dbscan = DBSCAN(eps=0.5, min_samples=min(5, len(tfidf_matrix)//10))\n",
    "                dbscan_labels = dbscan.fit_predict(scaled_matrix)\n",
    "                if len(set(dbscan_labels)) > 1:  # Ensure more than one cluster\n",
    "                    results['DBSCAN'] = {\n",
    "                        'labels': dbscan_labels,\n",
    "                        'silhouette_score': silhouette_score(tfidf_matrix, dbscan_labels)\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(\"DBSCAN encountered an issue:\", e)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Clustering error: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def classification_models(self, tfidf_matrix):\n",
    "        # More robust classification with additional processing\n",
    "        X = tfidf_matrix\n",
    "        y = self.combined_data['izin_encoded']\n",
    "        \n",
    "        # Split the data with stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features for better performance\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Multiple classification models\n",
    "        models = {\n",
    "            'SVM': SVC(kernel='linear', class_weight='balanced'),\n",
    "            'Random Forest': RandomForestClassifier(\n",
    "                n_estimators=100, \n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "                \n",
    "                # Fit the model\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                results[name] = {\n",
    "                    'cross_val_scores': cv_scores,\n",
    "                    'mean_cv_score': cv_scores.mean(),\n",
    "                    'classification_report': classification_report(y_test, y_pred)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {name} classification: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def deep_learning_approach(self):\n",
    "        # More memory-efficient deep learning approach\n",
    "        try:\n",
    "            # Tokenization with more robust parameters\n",
    "            tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')\n",
    "            tokenizer.fit_on_texts(self.combined_data['keperluan_cleaned'])\n",
    "            \n",
    "            # Sequence preparation\n",
    "            sequences = tokenizer.texts_to_sequences(self.combined_data['keperluan_cleaned'])\n",
    "            padded_sequences = pad_sequences(sequences, maxlen=50, padding='post', truncating='post')\n",
    "            \n",
    "            # Prepare labels (convert to categorical)\n",
    "            y = to_categorical(self.combined_data['izin_encoded'])\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                padded_sequences, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            # More complex LSTM Model\n",
    "            model = Sequential([\n",
    "                Embedding(1000, 64, input_length=50),\n",
    "                LSTM(128, return_sequences=True),\n",
    "                LSTM(64),\n",
    "                Dense(32, activation='relu'),\n",
    "                Dense(y.shape[1], activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # Reduce memory usage with smaller batches and early stopping\n",
    "            from tensorflow.keras.callbacks import EarlyStopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=3, \n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            # Train with reduced epochs and verbose output\n",
    "            history = model.fit(\n",
    "                X_train, y_train, \n",
    "                epochs=20, \n",
    "                batch_size=32,\n",
    "                validation_split=0.2, \n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1  # Change to 1 to see progress\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            \n",
    "            return {\n",
    "                'model': model,\n",
    "                'loss': loss,\n",
    "                'accuracy': accuracy,\n",
    "                'training_history': history\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Deep Learning approach error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def visualize_results(self, clustering_results, classification_results):\n",
    "        # Enhanced visualization with error handling\n",
    "        try:\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            \n",
    "            # Silhouette Scores\n",
    "            plt.subplot(1, 2, 1)\n",
    "            silhouette_data = [results.get('silhouette_score', 0) for results in clustering_results.values()]\n",
    "            plt.bar(clustering_results.keys(), silhouette_data)\n",
    "            plt.title('Clustering Silhouette Scores')\n",
    "            plt.ylabel('Silhouette Score')\n",
    "            plt.ylim(0, 1)\n",
    "            \n",
    "            # Classification Accuracy\n",
    "            plt.subplot(1, 2, 2)\n",
    "            cv_scores = [results.get('mean_cv_score', 0) for results in classification_results.values()]\n",
    "            plt.bar(classification_results.keys(), cv_scores)\n",
    "            plt.title('Classification Cross-Validation Scores')\n",
    "            plt.ylabel('Mean CV Score')\n",
    "            plt.ylim(0, 1)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Visualization error: {e}\")\n",
    "    \n",
    "    def identify_business_opportunities(self):\n",
    "        # Extract and analyze business opportunities from text\n",
    "        try:\n",
    "            # Group by purpose and count\n",
    "            purpose_counts = self.combined_data['keperluan_cleaned'].value_counts()\n",
    "            \n",
    "            # Top purposes\n",
    "            top_purposes = purpose_counts.head(10)\n",
    "            \n",
    "            print(\"\\nTop Business Opportunity Suggestions:\")\n",
    "            for purpose, count in top_purposes.items():\n",
    "                print(f\"Purpose: {purpose}, Frequency: {count}\")\n",
    "            \n",
    "            return top_purposes\n",
    "        except Exception as e:\n",
    "            print(f\"Business opportunity analysis error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def run_project(self):\n",
    "        # Comprehensive project execution with error handling\n",
    "        try:\n",
    "            # Feature Extraction\n",
    "            tfidf_matrix, vectorizer = self.feature_extraction()\n",
    "            \n",
    "            # Clustering Approaches\n",
    "            clustering_results = self.multiple_clustering_approaches(tfidf_matrix)\n",
    "            \n",
    "            # Classification Models\n",
    "            classification_results = self.classification_models(tfidf_matrix)\n",
    "            \n",
    "            # Deep Learning Approach\n",
    "            deep_learning_results = self.deep_learning_approach()\n",
    "            \n",
    "            # Visualize Results\n",
    "            self.visualize_results(clustering_results, classification_results)\n",
    "            \n",
    "            # Identify Business Opportunities\n",
    "            business_opportunities = self.identify_business_opportunities()\n",
    "            \n",
    "            return {\n",
    "                'clustering': clustering_results,\n",
    "                'classification': classification_results,\n",
    "                'deep_learning': deep_learning_results,\n",
    "                'business_opportunities': business_opportunities\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Project execution error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Jupyter Notebook Execution\n",
    "def main():\n",
    "    file_path = \"dataset.xlsx\"\n",
    "    project = AdvancedMLProject(file_path)\n",
    "    results = project.run_project()\n",
    "    \n",
    "    if results:\n",
    "        # Detailed Results Printing\n",
    "        print(\"\\n--- Detailed Project Results ---\")\n",
    "        \n",
    "        print(\"\\nClustering Results:\")\n",
    "        for algo, result in results['clustering'].items():\n",
    "            print(f\"{algo} - Silhouette Score: {result.get('silhouette_score', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\nClassification Results:\")\n",
    "        for model, result in results['classification'].items():\n",
    "            print(f\"{model}:\")\n",
    "            print(f\"  Mean CV Score: {result['mean_cv_score']}\")\n",
    "            print(\"  Classification Report:\")\n",
    "            print(result['classification_report'])\n",
    "        \n",
    "        print(\"\\nDeep Learning Results:\")\n",
    "        print(f\"Accuracy: {results['deep_learning'].get('accuracy', 'N/A')}\")\n",
    "        print(f\"Loss: {results['deep_learning'].get('loss', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\nBusiness Opportunity Suggestions:\")\n",
    "        print(results.get('business_opportunities', 'No opportunities found'))\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecde211-e7cc-4ce5-b5fa-e57bd812eb62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
