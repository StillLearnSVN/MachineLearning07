{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63786168-77b7-44e7-8b27-1616e5f7083b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization: Memory usage: 43.9%\n",
      "Reading file: dataset.xlsx\n",
      "Data loaded and combined successfully.\n",
      "Starting feature extraction...\n",
      "Feature extraction completed.\n",
      "Performing DBSCAN clustering...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Data Processing and ML Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Additional Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Debugging Utilities\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Ensure NLTK downloads are available\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "\n",
    "def log_memory(stage):\n",
    "    \"\"\"Log memory usage at a specific stage.\"\"\"\n",
    "    print(f\"{stage}: Memory usage: {psutil.virtual_memory().percent}%\")\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "class DBSCANClustering:\n",
    "    def __init__(self, file_path):\n",
    "        log_memory(\"Initialization\")\n",
    "        self.file_path = file_path\n",
    "        self.load_and_preprocess_data()\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"Load data from Excel and preprocess columns.\"\"\"\n",
    "        print(f\"Reading file: {self.file_path}\")\n",
    "        try:\n",
    "            ib_data = pd.read_excel(self.file_path, sheet_name='DATA IB')\n",
    "            ik_data = pd.read_excel(self.file_path, sheet_name='DATA IK')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return\n",
    "        \n",
    "        ib_data['source'] = 'IB'\n",
    "        ik_data['source'] = 'IK'\n",
    "        ib_data['purpose'] = ib_data['desc']\n",
    "        ik_data['purpose'] = ik_data['tujuan']\n",
    "        self.combined_data = pd.concat(\n",
    "            [ib_data[['purpose', 'source']], ik_data[['purpose', 'source']]],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        self.combined_data = self.combined_data.dropna(subset=['purpose'])\n",
    "        print(\"Data loaded and combined successfully.\")\n",
    "\n",
    "    def advanced_text_preprocessing(self, text):\n",
    "        \"\"\"Clean and preprocess text for analysis.\"\"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('indonesian'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def feature_extraction(self):\n",
    "        \"\"\"Extract features using TF-IDF vectorization.\"\"\"\n",
    "        print(\"Starting feature extraction...\")\n",
    "        self.combined_data['processed_purpose'] = self.combined_data['purpose'].apply(self.advanced_text_preprocessing)\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        features = vectorizer.fit_transform(self.combined_data['processed_purpose']).toarray()\n",
    "        print(\"Feature extraction completed.\")\n",
    "        return features, vectorizer.get_feature_names_out()\n",
    "\n",
    "    def perform_dbscan_clustering_with_optimization(self, features, feature_names):\n",
    "        \"\"\"Perform DBSCAN clustering and optimize parameters.\"\"\"\n",
    "        print(\"Performing DBSCAN clustering...\")\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_features = pca.fit_transform(features)\n",
    "        \n",
    "        eps_range = np.linspace(0.5, 2.0, 10)\n",
    "        min_samples_range = range(3, 10)\n",
    "        \n",
    "        best_silhouette = -1\n",
    "        best_params = None\n",
    "        best_labels = None\n",
    "        \n",
    "        for eps in eps_range:\n",
    "            for min_samples in min_samples_range:\n",
    "                try:\n",
    "                    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                    labels = dbscan.fit_predict(features)\n",
    "                    if len(set(labels)) <= 1 or (len(set(labels)) == 2 and -1 in set(labels)):\n",
    "                        continue\n",
    "                    \n",
    "                    silhouette = silhouette_score(features, labels)\n",
    "                    if silhouette > best_silhouette:\n",
    "                        best_silhouette = silhouette\n",
    "                        best_params = (eps, min_samples)\n",
    "                        best_labels = labels\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with eps={eps}, min_samples={min_samples}: {e}\")\n",
    "        \n",
    "        if best_params:\n",
    "            print(f\"Best Parameters: eps={best_params[0]}, min_samples={best_params[1]}\")\n",
    "            print(f\"Best Silhouette Score: {best_silhouette}\")\n",
    "        else:\n",
    "            print(\"No valid clustering found.\")\n",
    "            return None\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(121)\n",
    "        scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=best_labels, cmap='viridis', s=15)\n",
    "        plt.title(\"DBSCAN Clustering with Best Parameters\")\n",
    "        plt.colorbar(scatter)\n",
    "\n",
    "        plt.subplot(122)\n",
    "        cluster_counts = pd.Series(best_labels).value_counts()\n",
    "        cluster_counts = cluster_counts[cluster_counts.index != -1]\n",
    "        cluster_counts.plot(kind='bar', color='blue')\n",
    "        plt.title(\"Cluster Distribution\")\n",
    "        plt.xlabel(\"Cluster\")\n",
    "        plt.ylabel(\"Number of Samples\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('dbscan_optimized_visualization.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return self.generate_business_insights(features, best_labels, feature_names)\n",
    "\n",
    "    def generate_business_insights(self, features, labels, feature_names):\n",
    "        \"\"\"Generate business insights based on clustering.\"\"\"\n",
    "        insights = {}\n",
    "        for cluster in np.unique(labels):\n",
    "            if cluster == -1:\n",
    "                continue\n",
    "            cluster_indices = np.where(labels == cluster)[0]\n",
    "            cluster_features = features[cluster_indices]\n",
    "            top_keyword_indices = cluster_features.sum(axis=0).argsort()[-5:][::-1]\n",
    "            top_keywords = [feature_names[i] for i in top_keyword_indices]\n",
    "            insights[cluster] = {\n",
    "                'keywords': top_keywords,\n",
    "                'sample_purposes': self.combined_data.iloc[cluster_indices]['purpose'].sample(min(3, len(cluster_indices))).tolist(),\n",
    "                'business_opportunities': self.map_keywords_to_opportunities(top_keywords)\n",
    "            }\n",
    "        return insights\n",
    "\n",
    "    def map_keywords_to_opportunities(self, keywords):\n",
    "        \"\"\"Map keywords to business opportunities.\"\"\"\n",
    "        opportunity_mappings = {\n",
    "            'laptop': ['Computer Repair Shop', 'Laptop Sales and Service', 'Tech Accessory Store'],\n",
    "            'orang': ['Family Support Services', 'Travel Assistance', 'Family Reunion Planning'],\n",
    "            'market': ['Market Research Consultancy', 'Local Business Consulting', 'Research Services'],\n",
    "            'ibadah': ['Event Management', 'Religious Event Planning', 'Community Event Services'],\n",
    "            'dinas': ['Government Liaison Services', 'Permit and Documentation Assistance']\n",
    "        }\n",
    "        opportunities = []\n",
    "        for keyword in keywords:\n",
    "            if keyword in opportunity_mappings:\n",
    "                opportunities.extend(opportunity_mappings[keyword])\n",
    "        return list(set(opportunities))\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_path = 'dataset.xlsx'\n",
    "    analysis = DBSCANClustering(file_path)\n",
    "    features, feature_names = analysis.feature_extraction()\n",
    "    results = analysis.perform_dbscan_clustering_with_optimization(features, feature_names)\n",
    "\n",
    "    if results:\n",
    "        print(\"\\n--- DBSCAN Clustering Business Insights ---\")\n",
    "        for cluster, cluster_info in results.items():\n",
    "            print(f\"\\nCluster {cluster}:\")\n",
    "            print(f\"Top Keywords: {cluster_info['keywords']}\")\n",
    "            print(\"Sample Purposes:\")\n",
    "            for purpose in cluster_info['sample_purposes']:\n",
    "                print(f\"- {purpose}\")\n",
    "            print(\"Potential Business Opportunities:\")\n",
    "            for opportunity in cluster_info['business_opportunities']:\n",
    "                print(f\"- {opportunity}\")\n",
    "\n",
    "        with open('dbscan_business_insights.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(\"Results saved to 'dbscan_business_insights.json'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6216803-0b58-4607-add1-2287acf9812a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
