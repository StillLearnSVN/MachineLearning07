{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a83b1e8-539b-4e3e-81de-facf27aa7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of clusters: 10\n",
      "\n",
      "--- K-Means Clustering Business Insights ---\n",
      "\n",
      "Cluster 0:\n",
      "Top Keywords: ['keluarga', 'bertemu', 'acara', 'berkunjung', 'rumah']\n",
      "Sample Purposes:\n",
      "- Ingin bertemu dengan keluarga dan ingin libur UAS\n",
      "- Rumah keluarga\n",
      "- Berkunjung ke tempat keluarga\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 1:\n",
      "Top Keywords: ['kerumah', 'pulang', 'keluarga', 'orangtua', 'kakak']\n",
      "Sample Purposes:\n",
      "- pulang kerumah\n",
      "- Balik kerumah\n",
      "- kembali kerumah\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 2:\n",
      "Top Keywords: ['pulang', 'rumah', 'keluarga', 'saudara', 'libur']\n",
      "Sample Purposes:\n",
      "- Pulang ke rumah\n",
      "- Pulang ke rumah\n",
      "- Pulang ke rumah\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 3:\n",
      "Top Keywords: ['lebaran', 'libur', 'liburan', 'keluarga', 'bertemu']\n",
      "Sample Purposes:\n",
      "- Libur lebaran dan bertemu keluarga\n",
      "- Libur Lebaran\n",
      "- Libur lebaran\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 4:\n",
      "Top Keywords: ['paskah', 'libur', 'merayakan', 'keluarga', 'perayaan']\n",
      "Sample Purposes:\n",
      "- Merayakan paskah\n",
      "- Paskah\n",
      "- paskah\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 5:\n",
      "Top Keywords: ['orangtua', 'bertemu', 'rumah', 'mengunjungi', 'pulang']\n",
      "Sample Purposes:\n",
      "- Bertemu dengan Orangtua\n",
      "- Bertemu dengan orangtua\n",
      "- Pulang ke rumah orangtua \n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 6:\n",
      "Top Keywords: ['rumah', 'bermalam', 'exit', 'clearance', 'izin']\n",
      "Sample Purposes:\n",
      "- Pulang ke rumah teman\n",
      "- Acara tunangan saudara perempuan \n",
      "- Periksa behel gigi\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 7:\n",
      "Top Keywords: ['semester', 'libur', 'magang', 'mbkm', 'liburan']\n",
      "Sample Purposes:\n",
      "- Libur akhir semester\n",
      "- Libur akhir semester\n",
      "- Libur akhir semester.\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 8:\n",
      "Top Keywords: ['libur', 'natal', 'idul', 'raya', 'imlek']\n",
      "Sample Purposes:\n",
      "- Libur dan magang\n",
      "- Libur\n",
      "- Libur \n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 9:\n",
      "Top Keywords: ['tua', 'orang', 'bertemu', 'rumah', 'pulang']\n",
      "Sample Purposes:\n",
      "- Bertemu orang tua\n",
      "- Bertemu orang tua\n",
      "- Bertemu Keluarga dan Orang tua\n",
      "Potential Business Opportunities:\n",
      "- Family Support Services\n",
      "- Travel Assistance\n",
      "- Family Reunion Planning\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Processing and ML Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Additional Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class KMeansClustering:\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize the K-Means clustering analysis\n",
    "        \n",
    "        Parameters:\n",
    "        file_path (str): Path to the Excel file\n",
    "        \"\"\"\n",
    "        # Download NLTK resources\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        self.load_and_preprocess_data(file_path)\n",
    "    \n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess data from Excel file\n",
    "        \"\"\"\n",
    "        # Read Excel sheets\n",
    "        ib_data = pd.read_excel(file_path, sheet_name='DATA IB')\n",
    "        ik_data = pd.read_excel(file_path, sheet_name='DATA IK')\n",
    "        \n",
    "        # Combine purposes\n",
    "        ib_data['source'] = 'IB'\n",
    "        ik_data['source'] = 'IK'\n",
    "        ib_data['purpose'] = ib_data['desc']\n",
    "        ik_data['purpose'] = ik_data['tujuan']\n",
    "        \n",
    "        # Merge datasets\n",
    "        self.combined_data = pd.concat([\n",
    "            ib_data[['purpose', 'source']], \n",
    "            ik_data[['purpose', 'source']]\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Remove NaN values\n",
    "        self.combined_data = self.combined_data.dropna(subset=['purpose'])\n",
    "    \n",
    "    def advanced_text_preprocessing(self, text):\n",
    "        \"\"\"\n",
    "        Advanced text preprocessing\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenization\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except:\n",
    "            tokens = text.split()\n",
    "        \n",
    "        # Remove stopwords\n",
    "        try:\n",
    "            stop_words = set(stopwords.words('indonesian'))\n",
    "        except:\n",
    "            stop_words = set()\n",
    "        \n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def feature_extraction(self):\n",
    "        \"\"\"\n",
    "        Extract features from text data\n",
    "        \n",
    "        Returns:\n",
    "        numpy array: Feature matrix\n",
    "        list: Feature names\n",
    "        \"\"\"\n",
    "        # Preprocess text\n",
    "        self.combined_data['processed_purpose'] = self.combined_data['purpose'].apply(self.advanced_text_preprocessing)\n",
    "        \n",
    "        # Feature extraction using TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "        feature_matrix = vectorizer.fit_transform(self.combined_data['processed_purpose'])\n",
    "        \n",
    "        # Store feature names for interpretation\n",
    "        self.feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        return feature_matrix.toarray(), self.feature_names\n",
    "    \n",
    "    def perform_kmeans_clustering(self, features, feature_names):\n",
    "        \"\"\"\n",
    "        Perform K-Means clustering with optimization\n",
    "        \n",
    "        Parameters:\n",
    "        features (numpy array): Feature matrix\n",
    "        feature_names (list): Feature names\n",
    "        \n",
    "        Returns:\n",
    "        dict: Clustering results and insights\n",
    "        \"\"\"\n",
    "        # Dimensionality reduction for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_features = pca.fit_transform(features)\n",
    "        \n",
    "        # Evaluate different numbers of clusters\n",
    "        max_clusters = min(10, len(features) // 2)  # Limit max clusters\n",
    "        silhouette_scores = []\n",
    "        \n",
    "        # Reduce computation time by using a smaller range\n",
    "        cluster_range = range(2, max_clusters + 1)\n",
    "        \n",
    "        for n_clusters in cluster_range:\n",
    "            try:\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                cluster_labels = kmeans.fit_predict(features)\n",
    "                \n",
    "                # Calculate silhouette score\n",
    "                silhouette = silhouette_score(features, cluster_labels)\n",
    "                silhouette_scores.append(silhouette)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {n_clusters} clusters: {e}\")\n",
    "                silhouette_scores.append(-1)\n",
    "        \n",
    "        # Visualize Silhouette Scores\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(list(cluster_range), silhouette_scores, marker='o')\n",
    "        plt.title('K-Means Silhouette Scores')\n",
    "        plt.xlabel('Number of Clusters')\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('kmeans_silhouette_scores.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Select best number of clusters\n",
    "        best_n_clusters = list(cluster_range)[np.argmax(silhouette_scores)]\n",
    "        print(f\"Best number of clusters: {best_n_clusters}\")\n",
    "        \n",
    "        # Perform clustering with best number of clusters\n",
    "        kmeans = KMeans(n_clusters=best_n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(features)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Scatter plot of clusters\n",
    "        plt.subplot(121)\n",
    "        scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                              c=cluster_labels, cmap='viridis')\n",
    "        plt.title('K-Means Clustering')\n",
    "        plt.colorbar(scatter)\n",
    "        \n",
    "        # Cluster distribution\n",
    "        plt.subplot(122)\n",
    "        cluster_counts = pd.Series(cluster_labels).value_counts()\n",
    "        cluster_counts.plot(kind='bar')\n",
    "        plt.title('Cluster Distribution')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('kmeans_clustering_visualization.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Generate business insights\n",
    "        return self.generate_business_insights(\n",
    "            features, cluster_labels, feature_names\n",
    "        )\n",
    "    \n",
    "    def generate_business_insights(self, features, cluster_labels, feature_names):\n",
    "        \"\"\"\n",
    "        Generate business insights from clustered data\n",
    "        \"\"\"\n",
    "        # Add cluster labels to dataframe\n",
    "        self.combined_data['cluster'] = cluster_labels\n",
    "        \n",
    "        # Analyze clusters\n",
    "        cluster_insights = {}\n",
    "        \n",
    "        for cluster in np.unique(cluster_labels):\n",
    "            cluster_data = self.combined_data[self.combined_data['cluster'] == cluster]\n",
    "            \n",
    "            # Find top keywords for the cluster\n",
    "            cluster_features = features[cluster_labels == cluster]\n",
    "            top_keyword_indices = cluster_features.sum(axis=0).argsort()[::-1][:5]\n",
    "            top_keywords = [feature_names[i] for i in top_keyword_indices]\n",
    "            \n",
    "            cluster_insights[cluster] = {\n",
    "                'keywords': top_keywords,\n",
    "                'sample_purposes': cluster_data['purpose'].sample(min(3, len(cluster_data))).tolist(),\n",
    "                'business_opportunities': self.map_keywords_to_opportunities(top_keywords)\n",
    "            }\n",
    "        \n",
    "        return cluster_insights\n",
    "    \n",
    "    def map_keywords_to_opportunities(self, keywords):\n",
    "        \"\"\"\n",
    "        Map keywords to potential business opportunities\n",
    "        \"\"\"\n",
    "        opportunity_mappings = {\n",
    "            'laptop': ['Computer Repair Shop', 'Laptop Sales and Service', 'Tech Accessory Store'],\n",
    "            'orang': ['Family Support Services', 'Travel Assistance', 'Family Reunion Planning'],\n",
    "            'market': ['Market Research Consultancy', 'Local Business Consulting', 'Research Services'],\n",
    "            'ibadah': ['Event Management', 'Religious Event Planning', 'Community Event Services'],\n",
    "            'dinas': ['Government Liaison Services', 'Permit and Documentation Assistance']\n",
    "        }\n",
    "        \n",
    "        opportunities = []\n",
    "        for keyword in keywords:\n",
    "            if keyword in opportunity_mappings:\n",
    "                opportunities.extend(opportunity_mappings[keyword])\n",
    "        \n",
    "        return list(set(opportunities))\n",
    "\n",
    "def main():\n",
    "    # File path\n",
    "    file_path = 'dataset.xlsx'\n",
    "    \n",
    "    # Initialize K-Means clustering analysis\n",
    "    analysis = KMeansClustering(file_path)\n",
    "    \n",
    "    # Feature Extraction\n",
    "    features, feature_names = analysis.feature_extraction()\n",
    "    \n",
    "    # Perform K-Means Clustering\n",
    "    kmeans_results = analysis.perform_kmeans_clustering(features, feature_names)\n",
    "    \n",
    "    # Print Business Insights\n",
    "    print(\"\\n--- K-Means Clustering Business Insights ---\")\n",
    "    for cluster, cluster_info in kmeans_results.items():\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        print(f\"Top Keywords: {cluster_info['keywords']}\")\n",
    "        print(\"Sample Purposes:\")\n",
    "        for purpose in cluster_info['sample_purposes']:\n",
    "            print(f\"- {purpose}\")\n",
    "        print(\"Potential Business Opportunities:\")\n",
    "        for opportunity in cluster_info['business_opportunities']:\n",
    "            print(f\"- {opportunity}\")\n",
    "    \n",
    "    # Optional: Save results to a file\n",
    "    import json\n",
    "    with open('kmeans_business_insights.json', 'w') as f:\n",
    "        # Convert numpy keys to standard Python types\n",
    "        json.dump({int(k): v for k, v in kmeans_results.items()}, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437a038-bfc1-4bca-bb56-bebbe52773e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
