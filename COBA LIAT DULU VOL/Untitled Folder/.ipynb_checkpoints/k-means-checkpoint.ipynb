{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a83b1e8-539b-4e3e-81de-facf27aa7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of clusters: 10\n",
      "\n",
      "--- K-Means Clustering Business Insights ---\n",
      "\n",
      "Cluster 0:\n",
      "Top Keywords: ['keluarga', 'bertemu', 'acara', 'berkunjung', 'rumah']\n",
      "Sample Purposes:\n",
      "- Acara Keluarga\n",
      "- Bertemu Keluarga\n",
      "- Bertemu Keluarga\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 1:\n",
      "Top Keywords: ['kerumah', 'pulang', 'keluarga', 'orangtua', 'kakak']\n",
      "Sample Purposes:\n",
      "- pulang kerumah\n",
      "- Pulang kerumah \n",
      "- Pulang kerumah\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 2:\n",
      "Top Keywords: ['pulang', 'rumah', 'keluarga', 'saudara', 'libur']\n",
      "Sample Purposes:\n",
      "- Pulang ke rumah\n",
      "- Ingin pulang ke rumah\n",
      "- Pulang ke rumah\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 3:\n",
      "Top Keywords: ['lebaran', 'libur', 'liburan', 'keluarga', 'bertemu']\n",
      "Sample Purposes:\n",
      "- Libur Lebaran\n",
      "- Libur lebaran\n",
      "- Libur lebaran\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 4:\n",
      "Top Keywords: ['paskah', 'libur', 'merayakan', 'keluarga', 'perayaan']\n",
      "Sample Purposes:\n",
      "- ibadah paskah bersama keluarga \n",
      "- Libur Paskah\n",
      "- Libur Paskah\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 5:\n",
      "Top Keywords: ['orangtua', 'bertemu', 'rumah', 'mengunjungi', 'pulang']\n",
      "Sample Purposes:\n",
      "- Mengunjungi Orangtua\n",
      "- Bertemu Orangtua\n",
      "- Bertemu Orangtua\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 6:\n",
      "Top Keywords: ['rumah', 'bermalam', 'exit', 'clearance', 'izin']\n",
      "Sample Purposes:\n",
      "- Izin Bermalam di Rumah\n",
      "- Kusut\n",
      "- Lock down campus\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 7:\n",
      "Top Keywords: ['semester', 'libur', 'magang', 'mbkm', 'liburan']\n",
      "Sample Purposes:\n",
      "- Libur Semester\n",
      "- Libur semester\n",
      "- Libur Semester\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 8:\n",
      "Top Keywords: ['libur', 'natal', 'idul', 'raya', 'imlek']\n",
      "Sample Purposes:\n",
      "- Libur panjaaaaaaaaaang\n",
      "- Libur Natal dan Tahun Baru\n",
      "- libur imlek\n",
      "Potential Business Opportunities:\n",
      "\n",
      "Cluster 9:\n",
      "Top Keywords: ['tua', 'orang', 'bertemu', 'rumah', 'pulang']\n",
      "Sample Purposes:\n",
      "- Bertemu orang tua dan libur imlek\n",
      "- Mengunjungi orang tua\n",
      "- Bertemu orang tua\n",
      "Potential Business Opportunities:\n",
      "- Family Support Services\n",
      "- Travel Assistance\n",
      "- Family Reunion Planning\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 256\u001b[0m\n\u001b[0;32m    253\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(kmeans_results, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 253\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkmeans_business_insights.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 253\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkmeans_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[0;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[0;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[0;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\encoder.py:377\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeys must be str, int, float, bool or None, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    378\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first:\n\u001b[0;32m    380\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: keys must be str, int, float, bool or None, not int32"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Processing and ML Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Additional Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class KMeansClustering:\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize the K-Means clustering analysis\n",
    "        \n",
    "        Parameters:\n",
    "        file_path (str): Path to the Excel file\n",
    "        \"\"\"\n",
    "        # Download NLTK resources\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        self.load_and_preprocess_data(file_path)\n",
    "    \n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess data from Excel file\n",
    "        \"\"\"\n",
    "        # Read Excel sheets\n",
    "        ib_data = pd.read_excel(file_path, sheet_name='DATA IB')\n",
    "        ik_data = pd.read_excel(file_path, sheet_name='DATA IK')\n",
    "        \n",
    "        # Combine purposes\n",
    "        ib_data['source'] = 'IB'\n",
    "        ik_data['source'] = 'IK'\n",
    "        ib_data['purpose'] = ib_data['desc']\n",
    "        ik_data['purpose'] = ik_data['tujuan']\n",
    "        \n",
    "        # Merge datasets\n",
    "        self.combined_data = pd.concat([\n",
    "            ib_data[['purpose', 'source']], \n",
    "            ik_data[['purpose', 'source']]\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Remove NaN values\n",
    "        self.combined_data = self.combined_data.dropna(subset=['purpose'])\n",
    "    \n",
    "    def advanced_text_preprocessing(self, text):\n",
    "        \"\"\"\n",
    "        Advanced text preprocessing\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenization\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except:\n",
    "            tokens = text.split()\n",
    "        \n",
    "        # Remove stopwords\n",
    "        try:\n",
    "            stop_words = set(stopwords.words('indonesian'))\n",
    "        except:\n",
    "            stop_words = set()\n",
    "        \n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def feature_extraction(self):\n",
    "        \"\"\"\n",
    "        Extract features from text data\n",
    "        \n",
    "        Returns:\n",
    "        numpy array: Feature matrix\n",
    "        list: Feature names\n",
    "        \"\"\"\n",
    "        # Preprocess text\n",
    "        self.combined_data['processed_purpose'] = self.combined_data['purpose'].apply(self.advanced_text_preprocessing)\n",
    "        \n",
    "        # Feature extraction using TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "        feature_matrix = vectorizer.fit_transform(self.combined_data['processed_purpose'])\n",
    "        \n",
    "        # Store feature names for interpretation\n",
    "        self.feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        return feature_matrix.toarray(), self.feature_names\n",
    "    \n",
    "    def perform_kmeans_clustering(self, features, feature_names):\n",
    "        \"\"\"\n",
    "        Perform K-Means clustering with optimization\n",
    "        \n",
    "        Parameters:\n",
    "        features (numpy array): Feature matrix\n",
    "        feature_names (list): Feature names\n",
    "        \n",
    "        Returns:\n",
    "        dict: Clustering results and insights\n",
    "        \"\"\"\n",
    "        # Dimensionality reduction for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_features = pca.fit_transform(features)\n",
    "        \n",
    "        # Evaluate different numbers of clusters\n",
    "        max_clusters = min(10, len(features) // 2)  # Limit max clusters\n",
    "        silhouette_scores = []\n",
    "        \n",
    "        # Reduce computation time by using a smaller range\n",
    "        cluster_range = range(2, max_clusters + 1)\n",
    "        \n",
    "        for n_clusters in cluster_range:\n",
    "            try:\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                cluster_labels = kmeans.fit_predict(features)\n",
    "                \n",
    "                # Calculate silhouette score\n",
    "                silhouette = silhouette_score(features, cluster_labels)\n",
    "                silhouette_scores.append(silhouette)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {n_clusters} clusters: {e}\")\n",
    "                silhouette_scores.append(-1)\n",
    "        \n",
    "        # Visualize Silhouette Scores\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(list(cluster_range), silhouette_scores, marker='o')\n",
    "        plt.title('K-Means Silhouette Scores')\n",
    "        plt.xlabel('Number of Clusters')\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('kmeans_silhouette_scores.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Select best number of clusters\n",
    "        best_n_clusters = list(cluster_range)[np.argmax(silhouette_scores)]\n",
    "        print(f\"Best number of clusters: {best_n_clusters}\")\n",
    "        \n",
    "        # Perform clustering with best number of clusters\n",
    "        kmeans = KMeans(n_clusters=best_n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(features)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Scatter plot of clusters\n",
    "        plt.subplot(121)\n",
    "        scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                              c=cluster_labels, cmap='viridis')\n",
    "        plt.title('K-Means Clustering')\n",
    "        plt.colorbar(scatter)\n",
    "        \n",
    "        # Cluster distribution\n",
    "        plt.subplot(122)\n",
    "        cluster_counts = pd.Series(cluster_labels).value_counts()\n",
    "        cluster_counts.plot(kind='bar')\n",
    "        plt.title('Cluster Distribution')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('kmeans_clustering_visualization.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Generate business insights\n",
    "        return self.generate_business_insights(\n",
    "            features, cluster_labels, feature_names\n",
    "        )\n",
    "    \n",
    "    def generate_business_insights(self, features, cluster_labels, feature_names):\n",
    "        \"\"\"\n",
    "        Generate business insights from clustered data\n",
    "        \"\"\"\n",
    "        # Add cluster labels to dataframe\n",
    "        self.combined_data['cluster'] = cluster_labels\n",
    "        \n",
    "        # Analyze clusters\n",
    "        cluster_insights = {}\n",
    "        \n",
    "        for cluster in np.unique(cluster_labels):\n",
    "            cluster_data = self.combined_data[self.combined_data['cluster'] == cluster]\n",
    "            \n",
    "            # Find top keywords for the cluster\n",
    "            cluster_features = features[cluster_labels == cluster]\n",
    "            top_keyword_indices = cluster_features.sum(axis=0).argsort()[::-1][:5]\n",
    "            top_keywords = [feature_names[i] for i in top_keyword_indices]\n",
    "            \n",
    "            cluster_insights[cluster] = {\n",
    "                'keywords': top_keywords,\n",
    "                'sample_purposes': cluster_data['purpose'].sample(min(3, len(cluster_data))).tolist(),\n",
    "                'business_opportunities': self.map_keywords_to_opportunities(top_keywords)\n",
    "            }\n",
    "        \n",
    "        return cluster_insights\n",
    "    \n",
    "    def map_keywords_to_opportunities(self, keywords):\n",
    "        \"\"\"\n",
    "        Map keywords to potential business opportunities\n",
    "        \"\"\"\n",
    "        opportunity_mappings = {\n",
    "            'laptop': ['Computer Repair Shop', 'Laptop Sales and Service', 'Tech Accessory Store'],\n",
    "            'orang': ['Family Support Services', 'Travel Assistance', 'Family Reunion Planning'],\n",
    "            'market': ['Market Research Consultancy', 'Local Business Consulting', 'Research Services'],\n",
    "            'ibadah': ['Event Management', 'Religious Event Planning', 'Community Event Services'],\n",
    "            'dinas': ['Government Liaison Services', 'Permit and Documentation Assistance']\n",
    "        }\n",
    "        \n",
    "        opportunities = []\n",
    "        for keyword in keywords:\n",
    "            if keyword in opportunity_mappings:\n",
    "                opportunities.extend(opportunity_mappings[keyword])\n",
    "        \n",
    "        return list(set(opportunities))\n",
    "\n",
    "def main():\n",
    "    # File path\n",
    "    file_path = 'dataset.xlsx'\n",
    "    \n",
    "    # Initialize K-Means clustering analysis\n",
    "    analysis = KMeansClustering(file_path)\n",
    "    \n",
    "    # Feature Extraction\n",
    "    features, feature_names = analysis.feature_extraction()\n",
    "    \n",
    "    # Perform K-Means Clustering\n",
    "    kmeans_results = analysis.perform_kmeans_clustering(features, feature_names)\n",
    "    \n",
    "    # Print Business Insights\n",
    "    print(\"\\n--- K-Means Clustering Business Insights ---\")\n",
    "    for cluster, cluster_info in kmeans_results.items():\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        print(f\"Top Keywords: {cluster_info['keywords']}\")\n",
    "        print(\"Sample Purposes:\")\n",
    "        for purpose in cluster_info['sample_purposes']:\n",
    "            print(f\"- {purpose}\")\n",
    "        print(\"Potential Business Opportunities:\")\n",
    "        for opportunity in cluster_info['business_opportunities']:\n",
    "            print(f\"- {opportunity}\")\n",
    "    \n",
    "    # Optional: Save results to a file\n",
    "    import json\n",
    "    with open('kmeans_business_insights.json', 'w') as f:\n",
    "        json.dump(kmeans_results, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437a038-bfc1-4bca-bb56-bebbe52773e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
