{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2ac934-5c44-45f0-be47-624cb866593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Models\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, \n",
    "    adjusted_rand_score, \n",
    "    confusion_matrix, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Deep Learning (Optional)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc8598-def8-4db8-8d03-ef3647c4eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedModelEvaluation:\n",
    "    def __init__(self, file_path):\n",
    "        # Load data\n",
    "        self.ib_data = pd.read_excel(file_path, sheet_name=\"DATA IB\")\n",
    "        self.ik_data = pd.read_excel(file_path, sheet_name=\"DATA IK\")\n",
    "        \n",
    "        # Prepare data\n",
    "        self.prepare_data()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        # Combine and preprocess data\n",
    "        ib_data = self.ib_data[[\"desc\"]].rename(columns={\"desc\": \"keperluan\"})\n",
    "        ik_data = self.ik_data[[\"tujuan\"]].rename(columns={\"tujuan\": \"keperluan\"})\n",
    "        \n",
    "        ib_data[\"izin\"] = \"IB\"\n",
    "        ik_data[\"izin\"] = \"IK\"\n",
    "        \n",
    "        self.combined_data = pd.concat([ib_data, ik_data], ignore_index=True)\n",
    "        \n",
    "        # Clean text\n",
    "        self.combined_data['keperluan_cleaned'] = self.combined_data['keperluan'].str.lower().str.strip()\n",
    "        \n",
    "    def feature_extraction(self, method='tfidf'):\n",
    "        \"\"\"\n",
    "        Multiple feature extraction methods\n",
    "        \"\"\"\n",
    "        if method == 'tfidf':\n",
    "            # TF-IDF Vectorization\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                stop_words='english', \n",
    "                max_features=100, \n",
    "                ngram_range=(1,2)\n",
    "            )\n",
    "            features = vectorizer.fit_transform(self.combined_data['keperluan_cleaned'])\n",
    "        elif method == 'count':\n",
    "            # Count Vectorization\n",
    "            vectorizer = CountVectorizer(\n",
    "                stop_words='english', \n",
    "                max_features=100, \n",
    "                ngram_range=(1,2)\n",
    "            )\n",
    "            features = vectorizer.fit_transform(self.combined_data['keperluan_cleaned'])\n",
    "        \n",
    "        return features, vectorizer\n",
    "    \n",
    "    def clustering_models(self, features):\n",
    "        \"\"\"\n",
    "        Multiple Clustering Approaches\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # K-means Clustering\n",
    "        kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "        kmeans_labels = kmeans.fit_predict(features)\n",
    "        results['kmeans'] = {\n",
    "            'model': kmeans,\n",
    "            'labels': kmeans_labels,\n",
    "            'silhouette_score': silhouette_score(features, kmeans_labels)\n",
    "        }\n",
    "        \n",
    "        # DBSCAN Clustering\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        try:\n",
    "            dbscan_labels = dbscan.fit_predict(features)\n",
    "            results['dbscan'] = {\n",
    "                'model': dbscan,\n",
    "                'labels': dbscan_labels,\n",
    "                'silhouette_score': silhouette_score(features, dbscan_labels)\n",
    "            }\n",
    "        except:\n",
    "            print(\"DBSCAN clustering failed\")\n",
    "        \n",
    "        # Gaussian Mixture Model\n",
    "        gmm = GaussianMixture(n_components=5, random_state=42)\n",
    "        gmm_labels = gmm.fit_predict(features)\n",
    "        results['gmm'] = {\n",
    "            'model': gmm,\n",
    "            'labels': gmm_labels,\n",
    "            'silhouette_score': silhouette_score(features, gmm_labels)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def classification_models(self, features):\n",
    "        \"\"\"\n",
    "        Classification Models for Validation\n",
    "        \"\"\"\n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(self.combined_data['izin'])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Support Vector Machine\n",
    "        svm_params = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "        svm = SVC(random_state=42)\n",
    "        svm_grid = GridSearchCV(svm, svm_params, cv=3)\n",
    "        svm_grid.fit(X_train, y_train)\n",
    "        \n",
    "        # Random Forest\n",
    "        rf_params = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20]\n",
    "        }\n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        rf_grid = GridSearchCV(rf, rf_params, cv=3)\n",
    "        rf_grid.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\nSVM Classification Report:\")\n",
    "        svm_pred = svm_grid.predict(X_test)\n",
    "        print(classification_report(y_test, svm_pred))\n",
    "        \n",
    "        print(\"\\nRandom Forest Classification Report:\")\n",
    "        rf_pred = rf_grid.predict(X_test)\n",
    "        print(classification_report(y_test, rf_pred))\n",
    "        \n",
    "        return {\n",
    "            'svm': svm_grid,\n",
    "            'random_forest': rf_grid\n",
    "        }\n",
    "    \n",
    "    def deep_learning_approach(self):\n",
    "        \"\"\"\n",
    "        Optional Deep Learning Approach\n",
    "        \"\"\"\n",
    "        # Tokenization\n",
    "        tokenizer = Tokenizer(num_words=100)\n",
    "        tokenizer.fit_on_texts(self.combined_data['keperluan_cleaned'])\n",
    "        sequences = tokenizer.texts_to_sequences(self.combined_data['keperluan_cleaned'])\n",
    "        \n",
    "        # Pad sequences\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=20)\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(self.combined_data['izin'])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            padded_sequences, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Build LSTM Model\n",
    "        model = Sequential([\n",
    "            Embedding(100, 32, input_length=20),\n",
    "            LSTM(64),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam', \n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train, \n",
    "            epochs=10, \n",
    "            validation_split=0.2, \n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "        print(f\"\\nDeep Learning Model Accuracy: {test_acc}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def visualization(self, features):\n",
    "        \"\"\"\n",
    "        Dimensionality Reduction for Visualization\n",
    "        \"\"\"\n",
    "        # PCA for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_features = pca.fit_transform(features.toarray())\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(pca_features[:, 0], pca_features[:, 1])\n",
    "        plt.title('PCA Visualization of Features')\n",
    "        plt.xlabel('First Principal Component')\n",
    "        plt.ylabel('Second Principal Component')\n",
    "        plt.show()\n",
    "    \n",
    "    def run_evaluation(self):\n",
    "        \"\"\"\n",
    "        Comprehensive Model Evaluation\n",
    "        \"\"\"\n",
    "        # Feature Extraction\n",
    "        tfidf_features, tfidf_vectorizer = self.feature_extraction(method='tfidf')\n",
    "        count_features, count_vectorizer = self.feature_extraction(method='count')\n",
    "        \n",
    "        # Visualization\n",
    "        self.visualization(tfidf_features)\n",
    "        \n",
    "        # Clustering Models\n",
    "        print(\"\\nClustering Model Evaluation:\")\n",
    "        clustering_results = self.clustering_models(tfidf_features)\n",
    "        for name, result in clustering_results.items():\n",
    "            print(f\"{name.upper()} Silhouette Score: {result['silhouette_score']}\")\n",
    "        \n",
    "        # Classification Models\n",
    "        print(\"\\nClassification Model Evaluation:\")\n",
    "        classification_results = self.classification_models(tfidf_features)\n",
    "        \n",
    "        # Optional Deep Learning\n",
    "        # deep_learning_model = self.deep_learning_approach()\n",
    "        \n",
    "        return {\n",
    "            'clustering': clustering_results,\n",
    "            'classification': classification_results\n",
    "        }\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    file_path = \"dataset.xlsx\"\n",
    "    evaluator = AdvancedModelEvaluation(file_path)\n",
    "    results = evaluator.run_evaluation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541b5b58-0b23-4c00-8cdb-12cb82ea475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, silhouette_score\n",
    "\n",
    "# Keras/TensorFlow imports\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "# from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d67aa64-3f7f-4060-9ae3-e6da09bbc130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indonesian stopwords\n",
    "indonesian_stopwords = set([\n",
    "    'yang', 'di', 'ke', 'dari', 'pada', 'dalam', 'untuk', 'dengan', 'dan', 'atau',\n",
    "    'sebuah', 'seorang', 'ada', 'tidak', 'ini', 'itu', 'akan', 'dapat', 'saya', 'anda',\n",
    "    'dia', 'mereka', 'kita', 'tentang', 'sudah', 'jika', 'karena', 'adalah', 'bisa',\n",
    "    'saat', 'hal', 'mata', 'maka', 'kepada', 'setelah', 'sebagai', 'masih', 'seperti',\n",
    "    'sangat', 'telah', 'namun', 'jadi', 'melalui', 'apabila', 'sampai', 'lebih', 'selain'\n",
    "])\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess data from Excel file\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed combined data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load data\n",
    "        ib_data = pd.read_excel(file_path, sheet_name=\"DATA IB\")\n",
    "        ik_data = pd.read_excel(file_path, sheet_name=\"DATA IK\")\n",
    "        \n",
    "        # Prepare data\n",
    "        ib_data = ib_data[[\"desc\"]].rename(columns={\"desc\": \"keperluan\"})\n",
    "        ik_data = ik_data[[\"tujuan\"]].rename(columns={\"tujuan\": \"keperluan\"})\n",
    "        \n",
    "        ib_data[\"izin\"] = \"IB\"\n",
    "        ik_data[\"izin\"] = \"IK\"\n",
    "        \n",
    "        # Combine datasets\n",
    "        combined_data = pd.concat([ib_data, ik_data], ignore_index=True)\n",
    "        \n",
    "        # Text preprocessing with Indonesian-specific cleaning\n",
    "        def clean_text(text):\n",
    "            # Convert to lowercase\n",
    "            text = str(text).lower()\n",
    "            \n",
    "            # Remove punctuation and numbers\n",
    "            text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "            \n",
    "            # Remove stopwords\n",
    "            words = text.split()\n",
    "            words = [word for word in words if word not in indonesian_stopwords]\n",
    "            \n",
    "            return ' '.join(words)\n",
    "        \n",
    "        combined_data['keperluan_cleaned'] = combined_data['keperluan'].apply(clean_text)\n",
    "        \n",
    "        # Encode permit type\n",
    "        le = LabelEncoder()\n",
    "        combined_data['izin_encoded'] = le.fit_transform(combined_data['izin'])\n",
    "        \n",
    "        print(f\"Total combined data: {len(combined_data)}\")\n",
    "        print(f\"Unique permit types: {combined_data['izin'].unique()}\")\n",
    "        \n",
    "        return combined_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data loading and preprocessing: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e644908-79c4-4c62-9af1-9f4e9ff87e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined data: 80366\n",
      "Unique permit types: ['IB' 'IK']\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "file_path = \"dataset.xlsx\"\n",
    "combined_data = load_and_preprocess_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06a3753-1f52-42cf-aade-63a00276cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(combined_data, max_features=500):\n",
    "    \"\"\"\n",
    "    Extract TF-IDF features from cleaned text\n",
    "    \n",
    "    Args:\n",
    "        combined_data (pd.DataFrame): Preprocessed data\n",
    "        max_features (int): Maximum number of features\n",
    "    \n",
    "    Returns:\n",
    "        tuple: TF-IDF matrix and vectorizer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words=list(indonesian_stopwords),\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        tfidf_matrix = vectorizer.fit_transform(combined_data['keperluan_cleaned']).toarray()\n",
    "        \n",
    "        print(f\"TF-IDF Matrix shape: {tfidf_matrix.shape}\")\n",
    "        return tfidf_matrix, vectorizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature extraction: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a933e1d-c321-46a4-8e77-690ce62c3ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix shape: (80366, 500)\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "tfidf_matrix, vectorizer = extract_features(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e86579-13e5-44bb-b724-8fe63f89814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(tfidf_matrix):\n",
    "    \"\"\"\n",
    "    Perform multiple clustering approaches\n",
    "    \n",
    "    Args:\n",
    "        tfidf_matrix (np.ndarray): TF-IDF feature matrix\n",
    "    \n",
    "    Returns:\n",
    "        dict: Clustering results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # K-means Clustering\n",
    "        kmeans = KMeans(n_clusters=5, n_init=10, random_state=42)\n",
    "        kmeans_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "        results['KMeans'] = {\n",
    "            'labels': kmeans_labels,\n",
    "            'silhouette_score': silhouette_score(tfidf_matrix, kmeans_labels)\n",
    "        }\n",
    "        \n",
    "        # DBSCAN with adaptive parameters\n",
    "        try:\n",
    "            scaled_matrix = StandardScaler().fit_transform(tfidf_matrix)\n",
    "            dbscan = DBSCAN(eps=0.5, min_samples=min(5, len(tfidf_matrix)//10))\n",
    "            dbscan_labels = dbscan.fit_predict(scaled_matrix)\n",
    "            \n",
    "            if len(set(dbscan_labels)) > 1:\n",
    "                results['DBSCAN'] = {\n",
    "                    'labels': dbscan_labels,\n",
    "                    'silhouette_score': silhouette_score(tfidf_matrix, dbscan_labels)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(\"DBSCAN encountered an issue:\", e)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Clustering error: {e}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee37f99-4316-4036-9fc5-7149bb818362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "clustering_results = perform_clustering(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577f782d-3bdd-4c9e-b27f-92eb8f0918ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_classification(tfidf_matrix, combined_data):\n",
    "    \"\"\"\n",
    "    Perform classification using multiple models\n",
    "    \n",
    "    Args:\n",
    "        tfidf_matrix (np.ndarray): TF-IDF feature matrix\n",
    "        combined_data (pd.DataFrame): Preprocessed data\n",
    "    \n",
    "    Returns:\n",
    "        dict: Classification results\n",
    "    \"\"\"\n",
    "    X = tfidf_matrix\n",
    "    y = combined_data['izin_encoded']\n",
    "    \n",
    "    # Split the data with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Classification models\n",
    "    models = {\n",
    "        'SVM': SVC(kernel='linear', class_weight='balanced'),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "            \n",
    "            # Fit the model\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            results[name] = {\n",
    "                'cross_val_scores': cv_scores,\n",
    "                'mean_cv_score': cv_scores.mean(),\n",
    "                'classification_report': classification_report(y_test, y_pred)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {name} classification: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6582fba7-98b0-4c46-afb2-3ccf0b21e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "classification_results = perform_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f257e81-bc60-4ea6-98aa-77ec70773981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_business_opportunities(combined_data):\n",
    "    \"\"\"\n",
    "    Analyze and suggest business opportunities from text\n",
    "    \n",
    "    Args:\n",
    "        combined_data (pd.DataFrame): Preprocessed data\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Top business opportunities\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Group by purpose and count\n",
    "        purpose_counts = combined_data['keperluan_cleaned'].value_counts()\n",
    "        \n",
    "        # Top purposes\n",
    "        top_purposes = purpose_counts.head(15)\n",
    "        \n",
    "        print(\"\\nTop Business Opportunity Suggestions:\")\n",
    "        for purpose, count in top_purposes.items():\n",
    "            print(f\"Purpose: {purpose}, Frequency: {count}\")\n",
    "        \n",
    "        return top_purposes\n",
    "    except Exception as e:\n",
    "        print(f\"Business opportunity analysis error: {e}\")\n",
    "        return None\n",
    "\n",
    "def visualize_results(clustering_results, classification_results):\n",
    "    \"\"\"\n",
    "    Visualize clustering and classification results\n",
    "    \n",
    "    Args:\n",
    "        clustering_results (dict): Clustering results\n",
    "        classification_results (dict): Classification results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # Silhouette Scores\n",
    "        plt.subplot(1, 2, 1)\n",
    "        silhouette_data = [results.get('silhouette_score', 0) for results in clustering_results.values()]\n",
    "        plt.bar(clustering_results.keys(), silhouette_data)\n",
    "        plt.title('Clustering Silhouette Scores')\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Classification Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cv_scores = [results.get('mean_cv_score', 0) for results in classification_results.values()]\n",
    "        plt.bar(classification_results.keys(), cv_scores)\n",
    "        plt.title('Classification Cross-Validation Scores')\n",
    "        plt.ylabel('Mean CV Score')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error: {e}\")\n",
    "\n",
    "# Main execution function\n",
    "def run_complete_analysis(file_path):\n",
    "    \"\"\"\n",
    "    Run complete machine learning analysis\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive analysis results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load and Preprocess Data\n",
    "        combined_data = load_and_preprocess_data(file_path)\n",
    "        \n",
    "        # Step 2: Feature Extraction\n",
    "        tfidf_matrix, vectorizer = extract_features(combined_data)\n",
    "        \n",
    "        # Step 3: Clustering\n",
    "        clustering_results = perform_clustering(tfidf_matrix)\n",
    "        \n",
    "        # Step 4: Classification\n",
    "        classification_results = perform_classification(tfidf_matrix, combined_data)\n",
    "        \n",
    "        # Step 5: Deep Learning\n",
    "        # deep_learning_results = perform_deep_learning(combined_data)\n",
    "        \n",
    "        # Step 6: Business Opportunities\n",
    "        business_opportunities = identify_business_opportunities(combined_data)\n",
    "        \n",
    "        # Step 7: Visualization\n",
    "        visualize_results(clustering_results, classification_results)\n",
    "        \n",
    "        return {\n",
    "            'combined_data': combined_data,\n",
    "            'tfidf_matrix': tfidf_matrix,\n",
    "            'vectorizer': vectorizer,\n",
    "            'clustering': clustering_results,\n",
    "            'classification': classification_results,\n",
    "            # 'deep_learning': deep_learning_results,\n",
    "            'business_opportunities': business_opportunities\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Complete analysis error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948cb60-90c8-4fbd-b509-ec041bf42dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
