{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50edf226-0141-4df2-a98d-2d5d3b6478cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    silhouette_score\n",
    ")\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Deep Learning and Advanced Techniques\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89e72c-4640-4c61-a630-ffe0138cc3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMLProject:\n",
    "    def __init__(self, file_path):\n",
    "        # Load data\n",
    "        self.ib_data = pd.read_excel(file_path, sheet_name=\"DATA IB\")\n",
    "        self.ik_data = pd.read_excel(file_path, sheet_name=\"DATA IK\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        self.prepare_data()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # Combine and preprocess data\n",
    "        ib_data = self.ib_data[[\"desc\"]].rename(columns={\"desc\": \"keperluan\"})\n",
    "        ik_data = self.ik_data[[\"tujuan\"]].rename(columns={\"tujuan\": \"keperluan\"})\n",
    "        \n",
    "        ib_data[\"izin\"] = \"IB\"\n",
    "        ik_data[\"izin\"] = \"IK\"\n",
    "        \n",
    "        # Combine datasets\n",
    "        self.combined_data = pd.concat([ib_data, ik_data], ignore_index=True)\n",
    "        \n",
    "        # Text preprocessing\n",
    "        self.combined_data['keperluan_cleaned'] = self.combined_data['keperluan'].str.lower().str.strip()\n",
    "        \n",
    "        # Encode permit type\n",
    "        le = LabelEncoder()\n",
    "        self.combined_data['izin_encoded'] = le.fit_transform(self.combined_data['izin'])\n",
    "    \n",
    "    def feature_extraction(self):\n",
    "        # TF-IDF Vectorization\n",
    "        vectorizer = TfidfVectorizer(max_features=100)\n",
    "        tfidf_matrix = vectorizer.fit_transform(self.combined_data['keperluan_cleaned']).toarray()\n",
    "        \n",
    "        return tfidf_matrix, vectorizer\n",
    "    \n",
    "    def multiple_clustering_approaches(self, tfidf_matrix):\n",
    "        # Multiple clustering techniques\n",
    "        results = {}\n",
    "        \n",
    "        # K-means Clustering\n",
    "        kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "        kmeans_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "        results['KMeans'] = {\n",
    "            'labels': kmeans_labels,\n",
    "            'silhouette_score': silhouette_score(tfidf_matrix, kmeans_labels)\n",
    "        }\n",
    "        \n",
    "        # DBSCAN Clustering\n",
    "        try:\n",
    "            dbscan = DBSCAN(eps=0.5, min_samples=3)\n",
    "            dbscan_labels = dbscan.fit_predict(tfidf_matrix)\n",
    "            results['DBSCAN'] = {\n",
    "                'labels': dbscan_labels,\n",
    "                'silhouette_score': silhouette_score(tfidf_matrix, dbscan_labels)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(\"DBSCAN encountered an issue:\", e)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def classification_models(self, tfidf_matrix):\n",
    "        # Prepare data for classification\n",
    "        X = tfidf_matrix\n",
    "        y = self.combined_data['izin_encoded']\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Multiple classification models\n",
    "        models = {\n",
    "            'SVM': SVC(kernel='linear'),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "            \n",
    "            # Fit the model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            results[name] = {\n",
    "                'cross_val_scores': cv_scores,\n",
    "                'mean_cv_score': cv_scores.mean(),\n",
    "                'classification_report': classification_report(y_test, y_pred)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def deep_learning_approach(self):\n",
    "        # Tokenization\n",
    "        tokenizer = Tokenizer(num_words=100)\n",
    "        tokenizer.fit_on_texts(self.combined_data['keperluan_cleaned'])\n",
    "        \n",
    "        # Sequence preparation\n",
    "        sequences = tokenizer.texts_to_sequences(self.combined_data['keperluan_cleaned'])\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=20)\n",
    "        \n",
    "        # Prepare labels\n",
    "        y = self.combined_data['izin_encoded']\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            padded_sequences, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # LSTM Model\n",
    "        model = Sequential([\n",
    "            Embedding(100, 32, input_length=20),\n",
    "            LSTM(64),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train, \n",
    "            epochs=10, \n",
    "            validation_split=0.2, \n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy,\n",
    "            'training_history': history\n",
    "        }\n",
    "    \n",
    "    def visualize_results(self, clustering_results, classification_results):\n",
    "        # Visualize clustering results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Silhouette Scores\n",
    "        plt.subplot(1, 2, 1)\n",
    "        silhouette_data = [results['silhouette_score'] for results in clustering_results.values()]\n",
    "        plt.bar(clustering_results.keys(), silhouette_data)\n",
    "        plt.title('Clustering Silhouette Scores')\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        \n",
    "        # Classification Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cv_scores = [results['mean_cv_score'] for results in classification_results.values()]\n",
    "        plt.bar(classification_results.keys(), cv_scores)\n",
    "        plt.title('Classification Cross-Validation Scores')\n",
    "        plt.ylabel('Mean CV Score')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def run_project(self):\n",
    "        # Feature Extraction\n",
    "        tfidf_matrix, vectorizer = self.feature_extraction()\n",
    "        \n",
    "        # Clustering Approaches\n",
    "        clustering_results = self.multiple_clustering_approaches(tfidf_matrix)\n",
    "        \n",
    "        # Classification Models\n",
    "        classification_results = self.classification_models(tfidf_matrix)\n",
    "        \n",
    "        # Deep Learning Approach\n",
    "        deep_learning_results = self.deep_learning_approach()\n",
    "        \n",
    "        # Visualize Results\n",
    "        self.visualize_results(clustering_results, classification_results)\n",
    "        \n",
    "        return {\n",
    "            'clustering': clustering_results,\n",
    "            'classification': classification_results,\n",
    "            'deep_learning': deep_learning_results\n",
    "        }\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    file_path = \"dataset.xlsx\"\n",
    "    project = AdvancedMLProject(file_path)\n",
    "    results = project.run_project()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nClustering Results:\")\n",
    "    for algo, result in results['clustering'].items():\n",
    "        print(f\"{algo} - Silhouette Score: {result['silhouette_score']}\")\n",
    "    \n",
    "    print(\"\\nClassification Results:\")\n",
    "    for model, result in results['classification'].items():\n",
    "        print(f\"{model}:\")\n",
    "        print(f\"  Mean CV Score: {result['mean_cv_score']}\")\n",
    "        print(\"  Classification Report:\")\n",
    "        print(result['classification_report'])\n",
    "    \n",
    "    print(\"\\nDeep Learning Results:\")\n",
    "    print(f\"Accuracy: {results['deep_learning']['accuracy']}\")\n",
    "    print(f\"Loss: {results['deep_learning']['loss']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f374e7e-4feb-4ba4-b1c0-26fe2ab3f8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
